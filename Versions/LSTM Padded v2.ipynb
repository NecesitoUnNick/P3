{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3\n",
    "\n",
    "\n",
    "# Movie Genre Classification\n",
    "\n",
    "Classify a movie genre based on its plot.\n",
    "\n",
    "<img src=\"moviegenre.png\"\n",
    "     style=\"float: left; margin-right: 10px;\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "https://www.kaggle.com/c/miia4201-202019-p3-moviegenreclassification/overview\n",
    "\n",
    "### Data\n",
    "\n",
    "Input:\n",
    "- movie plot\n",
    "\n",
    "Output:\n",
    "Probability of the movie belong to each genre\n",
    "\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "- 20% API\n",
    "- 30% Report with all the details of the solution, the analysis and the conclusions. The report cannot exceed 10 pages, must be send in PDF format and must be self-contained.\n",
    "- 50% Performance in the Kaggle competition (The grade for each group will be proportional to the ranking it occupies in the competition. The group in the first place will obtain 5 points, for each position below, 0.25 points will be subtracted, that is: first place: 5 points, second: 4.75 points, third place: 4.50 points ... eleventh place: 2.50 points, twelfth place: 2.25 points).\n",
    "\n",
    "• The project must be carried out in the groups assigned for module 4.\n",
    "• Use clear and rigorous procedures.\n",
    "• The delivery of the project is on July 12, 2020, 11:59 pm, through Sicua + (Upload: the API and the report in PDF format).\n",
    "• No projects will be received after the delivery time or by any other means than the one established. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Acknowledgements\n",
    "\n",
    "We thank Professor Fabio Gonzalez, Ph.D. and his student John Arevalo for providing this dataset.\n",
    "\n",
    "See https://arxiv.org/abs/1702.01992"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuraciones Iniciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from time import time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk  import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, MinMaxScaler\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier,  ExtraTreesClassifier\n",
    "from sklearn.metrics import r2_score, roc_auc_score, accuracy_score, make_scorer, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import tensorflow as tf\n",
    "tf.device(\"gpu:0\")\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import Callback, EarlyStopping, TensorBoard\n",
    "from livelossplot import PlotLossesKeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTraining = pd.read_csv('datasets/dataTraining.csv', encoding='UTF-8', index_col=0)\n",
    "dataTesting = pd.read_csv('datasets/dataTesting.csv', encoding='UTF-8', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "      year                      title  \\\n3107  2003                       Most   \n900   2008  How to Be a Serial Killer   \n6724  1941             A Woman's Face   \n4704  1954            Executive Suite   \n2582  1990              Narrow Margin   \n\n                                                   plot  \\\n3107  most is the story of a single father who takes...   \n900   a serial killer decides to teach the secrets o...   \n6724  in sweden ,  a female blackmailer with a disfi...   \n4704  in a friday afternoon in new york ,  the presi...   \n2582  in los angeles ,  the editor of a publishing h...   \n\n                                  genres  rating  \n3107                  ['Short', 'Drama']     8.0  \n900        ['Comedy', 'Crime', 'Horror']     5.6  \n6724  ['Drama', 'Film-Noir', 'Thriller']     7.2  \n4704                           ['Drama']     7.4  \n2582     ['Action', 'Crime', 'Thriller']     6.6  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>title</th>\n      <th>plot</th>\n      <th>genres</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3107</th>\n      <td>2003</td>\n      <td>Most</td>\n      <td>most is the story of a single father who takes...</td>\n      <td>['Short', 'Drama']</td>\n      <td>8.0</td>\n    </tr>\n    <tr>\n      <th>900</th>\n      <td>2008</td>\n      <td>How to Be a Serial Killer</td>\n      <td>a serial killer decides to teach the secrets o...</td>\n      <td>['Comedy', 'Crime', 'Horror']</td>\n      <td>5.6</td>\n    </tr>\n    <tr>\n      <th>6724</th>\n      <td>1941</td>\n      <td>A Woman's Face</td>\n      <td>in sweden ,  a female blackmailer with a disfi...</td>\n      <td>['Drama', 'Film-Noir', 'Thriller']</td>\n      <td>7.2</td>\n    </tr>\n    <tr>\n      <th>4704</th>\n      <td>1954</td>\n      <td>Executive Suite</td>\n      <td>in a friday afternoon in new york ,  the presi...</td>\n      <td>['Drama']</td>\n      <td>7.4</td>\n    </tr>\n    <tr>\n      <th>2582</th>\n      <td>1990</td>\n      <td>Narrow Margin</td>\n      <td>in los angeles ,  the editor of a publishing h...</td>\n      <td>['Action', 'Crime', 'Thriller']</td>\n      <td>6.6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 111
    }
   ],
   "source": [
    "dataTraining.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   year                title  \\\n1  1999  Message in a Bottle   \n4  1978     Midnight Express   \n5  1996          Primal Fear   \n6  1950               Crisis   \n7  1959          The Tingler   \n\n                                                plot  \n1  who meets by fate ,  shall be sealed by fate ....  \n4  the true story of billy hayes ,  an american c...  \n5  martin vail left the chicago da ' s office to ...  \n6  husband and wife americans dr .  eugene and mr...  \n7  the coroner and scientist dr .  warren chapin ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>title</th>\n      <th>plot</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>1999</td>\n      <td>Message in a Bottle</td>\n      <td>who meets by fate ,  shall be sealed by fate ....</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1978</td>\n      <td>Midnight Express</td>\n      <td>the true story of billy hayes ,  an american c...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1996</td>\n      <td>Primal Fear</td>\n      <td>martin vail left the chicago da ' s office to ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1950</td>\n      <td>Crisis</td>\n      <td>husband and wife americans dr .  eugene and mr...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1959</td>\n      <td>The Tingler</td>\n      <td>the coroner and scientist dr .  warren chapin ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 112
    }
   ],
   "source": [
    "dataTesting.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdd(x):\n",
    "    a=x.split()\n",
    "    return len(a)\n",
    "\n",
    "import re\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_plot = []\n",
    "for sentance in dataTraining['plot'].values:\n",
    "#    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n",
    "    preprocessed_plot.append(sentance.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spaces(x):\n",
    "    x=x.split(\",\")\n",
    "    nospace=[]\n",
    "    for item in x:\n",
    "        item=item.lstrip()\n",
    "        nospace.append(item)\n",
    "    return (\",\").join(nospace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1863"
     },
     "metadata": {},
     "execution_count": 117
    }
   ],
   "source": [
    "max(dataTraining['plot'].apply(gdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "38707"
     },
     "metadata": {},
     "execution_count": 118
    }
   ],
   "source": [
    "vect=Tokenizer()\n",
    "vect.fit_on_texts(dataTraining['plot'])\n",
    "vocab_size = len(vect.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[  133     7     1 ...     0     0     0]\n [    4  1031   289 ...     0     0     0]\n [    6  8504     4 ...     0     0     0]\n ...\n [  126     4   829 ...     0     0     0]\n [  476 10126     4 ...     0     0     0]\n [38705  3226   490 ...     0     0     0]]\n"
    }
   ],
   "source": [
    "encoded_docs_train = vect.texts_to_sequences(dataTraining['plot'])\n",
    "max_length = vocab_size\n",
    "X_pad = sequence.pad_sequences(encoded_docs_train, maxlen=483, padding='post')\n",
    "print(X_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1micro(y_true, y_pred):\n",
    "    return tf.py_func(f1_score(y_true, y_pred,average='mirco'),tf.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 1, 0, 0],\n       ...,\n       [0, 1, 0, ..., 0, 0, 0],\n       [0, 1, 1, ..., 0, 0, 0],\n       [0, 1, 1, ..., 0, 0, 0]])"
     },
     "metadata": {},
     "execution_count": 121
    }
   ],
   "source": [
    "dataTraining['genres'] = dataTraining['genres'].map(lambda x: eval(x))\n",
    "le = MultiLabelBinarizer()\n",
    "y_genres = le.fit_transform(dataTraining['genres'])\n",
    "y_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_genres, y_test_genres = train_test_split(X_pad, y_genres, test_size=0.66, random_state=666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train multi-class multi-label model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"NLP - LSTM Genre Prediction \"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 483, 50)           24200     \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 483, 128)          91648     \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 483, 128)          0         \n_________________________________________________________________\nlstm_2 (LSTM)                (None, 64)                49408     \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 64)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 24)                1560      \n=================================================================\nTotal params: 166,816\nTrainable params: 166,816\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = Sequential(name=\"NLP - LSTM Genre Prediction \")\n",
    "# Configuring the parameters\n",
    "#model.add(Embedding(vocab_size, output_dim=50, input_length=1200))\n",
    "model.add(Embedding(484, output_dim=50, input_length=483))\n",
    "model.add(LSTM(128, return_sequences=True))  \n",
    "# Adding a dropout layer\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.5))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model.add(Dense(y_train_genres.shape[1], activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 2684 samples, validate on 5211 samples\nEpoch 1/5\n2684/2684 [==============================] - 77s 29ms/step - loss: nan - val_loss: nan\nEpoch 2/5\n2684/2684 [==============================] - 76s 28ms/step - loss: nan - val_loss: nan\nEpoch 3/5\n2684/2684 [==============================] - 76s 28ms/step - loss: nan - val_loss: nan\nEpoch 4/5\n2684/2684 [==============================] - 75s 28ms/step - loss: nan - val_loss: nan\nEpoch 5/5\n2684/2684 [==============================] - 75s 28ms/step - loss: nan - val_loss: nan\n"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "history = model.fit(X_train, y_train_genres,\n",
    "                    validation_data=[X_test, y_test_genres],\n",
    "                    epochs=5,\n",
    "                    batch_size=64,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 135
    }
   ],
   "source": [
    "y_pred_genres = model.predict_classes(X_test)\n",
    "y_pred_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.5"
     },
     "metadata": {},
     "execution_count": 129
    }
   ],
   "source": [
    "roc_auc_score(y_test_genres, y_pred_genres, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "X9 = dataTesting['plot'].to_list()\n",
    "#Convertimos todo en minúsculas\n",
    "X9 = [x.lower() for x in X9]\n",
    "X9 = [''.join(c for c in s if c not in string.punctuation) for s in X9]\n",
    "\n",
    "X8 = [x.split()[:len_pad] for x in X9]\n",
    "#Cambiamos las palabras por el valor int de esa palabra creado en el diccionario vocabulario\n",
    "X8 = [[voc[x1] for x1 in x if x1 in voc.keys()] for x in X8]\n",
    "#Creamos el PAD\n",
    "X_pad2 = sequence.pad_sequences(X8, maxlen=len_pad)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.100002  , 0.100002  , 0.100002  , ..., 0.10000685, 0.10001077,\n        0.10001326],\n       [0.100002  , 0.100002  , 0.100002  , ..., 0.10001112, 0.10001113,\n        0.10001607],\n       [0.100002  , 0.100002  , 0.100002  , ..., 0.10001343, 0.10000787,\n        0.10000312],\n       ...,\n       [0.100002  , 0.100002  , 0.100002  , ..., 0.10000414, 0.10001688,\n        0.10000942],\n       [0.100002  , 0.100002  , 0.100002  , ..., 0.10000227, 0.10001719,\n        0.10001163],\n       [0.100002  , 0.100002  , 0.100002  , ..., 0.10000577, 0.10000539,\n        0.10001248]])"
     },
     "metadata": {},
     "execution_count": 156
    }
   ],
   "source": [
    "X_pad2 = (X_pad2+5000)/50000\n",
    "X_pad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['p_Action', 'p_Adventure', 'p_Animation', 'p_Biography', 'p_Comedy', 'p_Crime', 'p_Documentary', 'p_Drama', 'p_Family',\n",
    "        'p_Fantasy', 'p_Film-Noir', 'p_History', 'p_Horror', 'p_Music', 'p_Musical', 'p_Mystery', 'p_News', 'p_Romance',\n",
    "        'p_Sci-Fi', 'p_Short', 'p_Sport', 'p_Thriller', 'p_War', 'p_Western']\n",
    "\n",
    "y_pred_test_genres = model.predict_proba(X_pad2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(y_pred_test_genres, index=dataTesting.index, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   p_Action  p_Adventure  p_Animation  p_Biography  p_Comedy   p_Crime  \\\n1  0.173973      0.14711     0.036689     0.046988  0.396658  0.196399   \n4  0.173973      0.14711     0.036689     0.046988  0.396658  0.196399   \n5  0.173973      0.14711     0.036689     0.046988  0.396658  0.196399   \n6  0.173973      0.14711     0.036689     0.046988  0.396658  0.196399   \n7  0.173973      0.14711     0.036689     0.046988  0.396658  0.196399   \n\n   p_Documentary   p_Drama  p_Family  p_Fantasy  ...  p_Musical  p_Mystery  \\\n1       0.052938  0.525232  0.090564   0.099127  ...   0.041329   0.086445   \n4       0.052938  0.525232  0.090564   0.099127  ...   0.041329   0.086445   \n5       0.052938  0.525232  0.090564   0.099127  ...   0.041329   0.086445   \n6       0.052938  0.525232  0.090564   0.099127  ...   0.041329   0.086445   \n7       0.052938  0.525232  0.090564   0.099127  ...   0.041329   0.086445   \n\n     p_News  p_Romance  p_Sci-Fi   p_Short   p_Sport  p_Thriller     p_War  \\\n1  0.005835   0.264969  0.085403  0.016008  0.039167    0.229333  0.042071   \n4  0.005835   0.264969  0.085403  0.016008  0.039167    0.229333  0.042071   \n5  0.005835   0.264969  0.085403  0.016008  0.039167    0.229333  0.042071   \n6  0.005835   0.264969  0.085403  0.016008  0.039167    0.229333  0.042071   \n7  0.005835   0.264969  0.085403  0.016008  0.039167    0.229333  0.042071   \n\n   p_Western  \n1    0.03236  \n4    0.03236  \n5    0.03236  \n6    0.03236  \n7    0.03236  \n\n[5 rows x 24 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>p_Action</th>\n      <th>p_Adventure</th>\n      <th>p_Animation</th>\n      <th>p_Biography</th>\n      <th>p_Comedy</th>\n      <th>p_Crime</th>\n      <th>p_Documentary</th>\n      <th>p_Drama</th>\n      <th>p_Family</th>\n      <th>p_Fantasy</th>\n      <th>...</th>\n      <th>p_Musical</th>\n      <th>p_Mystery</th>\n      <th>p_News</th>\n      <th>p_Romance</th>\n      <th>p_Sci-Fi</th>\n      <th>p_Short</th>\n      <th>p_Sport</th>\n      <th>p_Thriller</th>\n      <th>p_War</th>\n      <th>p_Western</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.173973</td>\n      <td>0.14711</td>\n      <td>0.036689</td>\n      <td>0.046988</td>\n      <td>0.396658</td>\n      <td>0.196399</td>\n      <td>0.052938</td>\n      <td>0.525232</td>\n      <td>0.090564</td>\n      <td>0.099127</td>\n      <td>...</td>\n      <td>0.041329</td>\n      <td>0.086445</td>\n      <td>0.005835</td>\n      <td>0.264969</td>\n      <td>0.085403</td>\n      <td>0.016008</td>\n      <td>0.039167</td>\n      <td>0.229333</td>\n      <td>0.042071</td>\n      <td>0.03236</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.173973</td>\n      <td>0.14711</td>\n      <td>0.036689</td>\n      <td>0.046988</td>\n      <td>0.396658</td>\n      <td>0.196399</td>\n      <td>0.052938</td>\n      <td>0.525232</td>\n      <td>0.090564</td>\n      <td>0.099127</td>\n      <td>...</td>\n      <td>0.041329</td>\n      <td>0.086445</td>\n      <td>0.005835</td>\n      <td>0.264969</td>\n      <td>0.085403</td>\n      <td>0.016008</td>\n      <td>0.039167</td>\n      <td>0.229333</td>\n      <td>0.042071</td>\n      <td>0.03236</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.173973</td>\n      <td>0.14711</td>\n      <td>0.036689</td>\n      <td>0.046988</td>\n      <td>0.396658</td>\n      <td>0.196399</td>\n      <td>0.052938</td>\n      <td>0.525232</td>\n      <td>0.090564</td>\n      <td>0.099127</td>\n      <td>...</td>\n      <td>0.041329</td>\n      <td>0.086445</td>\n      <td>0.005835</td>\n      <td>0.264969</td>\n      <td>0.085403</td>\n      <td>0.016008</td>\n      <td>0.039167</td>\n      <td>0.229333</td>\n      <td>0.042071</td>\n      <td>0.03236</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.173973</td>\n      <td>0.14711</td>\n      <td>0.036689</td>\n      <td>0.046988</td>\n      <td>0.396658</td>\n      <td>0.196399</td>\n      <td>0.052938</td>\n      <td>0.525232</td>\n      <td>0.090564</td>\n      <td>0.099127</td>\n      <td>...</td>\n      <td>0.041329</td>\n      <td>0.086445</td>\n      <td>0.005835</td>\n      <td>0.264969</td>\n      <td>0.085403</td>\n      <td>0.016008</td>\n      <td>0.039167</td>\n      <td>0.229333</td>\n      <td>0.042071</td>\n      <td>0.03236</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.173973</td>\n      <td>0.14711</td>\n      <td>0.036689</td>\n      <td>0.046988</td>\n      <td>0.396658</td>\n      <td>0.196399</td>\n      <td>0.052938</td>\n      <td>0.525232</td>\n      <td>0.090564</td>\n      <td>0.099127</td>\n      <td>...</td>\n      <td>0.041329</td>\n      <td>0.086445</td>\n      <td>0.005835</td>\n      <td>0.264969</td>\n      <td>0.085403</td>\n      <td>0.016008</td>\n      <td>0.039167</td>\n      <td>0.229333</td>\n      <td>0.042071</td>\n      <td>0.03236</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 24 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 159
    }
   ],
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv('pred_genres_text_RF.csv', index_label='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.100002  , 0.100002  , 0.100002  , ..., 0.10000685, 0.10001077,\n        0.10001326],\n       [0.100002  , 0.100002  , 0.100002  , ..., 0.10001112, 0.10001113,\n        0.10001607],\n       [0.100002  , 0.100002  , 0.100002  , ..., 0.10001343, 0.10000787,\n        0.10000312],\n       ...,\n       [0.100002  , 0.100002  , 0.100002  , ..., 0.10000414, 0.10001688,\n        0.10000942],\n       [0.100002  , 0.100002  , 0.100002  , ..., 0.10000227, 0.10001719,\n        0.10001163],\n       [0.100002  , 0.100002  , 0.100002  , ..., 0.10000577, 0.10000539,\n        0.10001248]])"
     },
     "metadata": {},
     "execution_count": 160
    }
   ],
   "source": [
    "X_pad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.     , 0.     , 0.     , ..., 0.2427 , 0.43874, 0.56278],\n       [0.     , 0.     , 0.     , ..., 0.45594, 0.45652, 0.7036 ],\n       [0.     , 0.     , 0.     , ..., 0.57136, 0.29344, 0.05602],\n       ...,\n       [0.     , 0.     , 0.     , ..., 0.1071 , 0.74382, 0.37094],\n       [0.     , 0.     , 0.     , ..., 0.0135 , 0.7597 , 0.48148],\n       [0.     , 0.     , 0.     , ..., 0.18866, 0.1696 , 0.5239 ]])"
     },
     "metadata": {},
     "execution_count": 228
    }
   ],
   "source": [
    "X9 = dataTesting['plot'].to_list()\n",
    "#Convertimos todo en minúsculas\n",
    "X9 = [x.lower() for x in X9]\n",
    "X9 = [''.join(c for c in s if c not in string.punctuation) for s in X9]\n",
    "#X9 = [word_tokenize(i) for i in X9]\n",
    "#x8 = [[word for word in X9[x] if not word in e_stp] for x in range(0,len(X9))]\n",
    "#X9 = [x for x in X9 if x in e_stp]\n",
    "\n",
    "X8 = [x.split()[:len_pad] for x in X9]\n",
    "#cambiamos las palabras por el valor int de esa palabra creado en el diccionario vocabulario\n",
    "X8 = [[voc[x1] for x1 in x if x1 in voc.keys()] for x in X8]\n",
    "#Creamos el PAD\n",
    "X_pad2 = (sequence.pad_sequences(X8, maxlen=len_pad))/50000\n",
    "X_pad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[0,\n 1,\n 2,\n 3,\n 4,\n 5,\n 6,\n 7,\n 8,\n 9,\n 10,\n 11,\n 12,\n 13,\n 14,\n 15,\n 16,\n 17,\n 18,\n 19,\n 20,\n 21,\n 22,\n 23,\n 24,\n 25,\n 26,\n 27,\n 28,\n 29,\n 30,\n 31,\n 32,\n 33,\n 34,\n 35,\n 36,\n 37,\n 38,\n 39,\n 40,\n 41,\n 42,\n 43,\n 44,\n 45,\n 46,\n 47,\n 48,\n 49,\n 50,\n 51,\n 52,\n 53,\n 54,\n 55,\n 56,\n 57,\n 58,\n 59,\n 60,\n 61,\n 62,\n 63,\n 64,\n 65,\n 66,\n 67,\n 68,\n 69,\n 70,\n 71,\n 72,\n 73,\n 74,\n 75,\n 76,\n 77,\n 78,\n 79,\n 80,\n 81,\n 82,\n 83,\n 84,\n 85,\n 86,\n 87,\n 88,\n 89,\n 90,\n 91,\n 92,\n 93,\n 94,\n 95,\n 96,\n 97,\n 98,\n 99,\n 100,\n 101,\n 102,\n 103,\n 104,\n 105,\n 106,\n 107,\n 108,\n 109,\n 110,\n 111,\n 112,\n 113,\n 114,\n 115,\n 116,\n 117,\n 118,\n 119,\n 120,\n 121,\n 122,\n 123,\n 124,\n 125,\n 126,\n 127,\n 128,\n 129,\n 130,\n 131,\n 132,\n 133,\n 134,\n 135,\n 136,\n 137,\n 138,\n 139,\n 140,\n 141,\n 142,\n 143,\n 144,\n 145,\n 146,\n 147,\n 148,\n 149,\n 150,\n 151,\n 152,\n 153,\n 154,\n 155,\n 156,\n 157,\n 158,\n 159,\n 160,\n 161,\n 162,\n 163,\n 164,\n 165,\n 166,\n 167,\n 168,\n 169,\n 170,\n 171,\n 172,\n 173,\n 174,\n 175,\n 176,\n 177,\n 178,\n 179,\n 180,\n 181,\n 182,\n 183,\n 184,\n 185,\n 186,\n 187,\n 188,\n 189,\n 190,\n 191,\n 192,\n 193,\n 194,\n 195,\n 196,\n 197,\n 198,\n 199,\n 200,\n 201,\n 202,\n 203,\n 204,\n 205,\n 206,\n 207,\n 208,\n 209,\n 210,\n 211,\n 212,\n 213,\n 214,\n 215,\n 216,\n 217,\n 218,\n 219,\n 220,\n 221,\n 222,\n 223,\n 224,\n 225,\n 226,\n 227,\n 228,\n 229,\n 230,\n 231,\n 232,\n 233,\n 234,\n 235,\n 236,\n 237,\n 238,\n 239,\n 240,\n 241,\n 242,\n 243,\n 244,\n 245,\n 246,\n 247,\n 248,\n 249,\n 250,\n 251,\n 252,\n 253,\n 254,\n 255,\n 256,\n 257,\n 258,\n 259,\n 260,\n 261,\n 262,\n 263,\n 264,\n 265,\n 266,\n 267,\n 268,\n 269,\n 270,\n 271,\n 272,\n 273,\n 274,\n 275,\n 276,\n 277,\n 278,\n 279,\n 280,\n 281,\n 282,\n 283,\n 284,\n 285,\n 286,\n 287,\n 288,\n 289,\n 290,\n 291,\n 292,\n 293,\n 294,\n 295,\n 296,\n 297,\n 298,\n 299,\n 300,\n 301,\n 302,\n 303,\n 304,\n 305,\n 306,\n 307,\n 308,\n 309,\n 310,\n 311,\n 312,\n 313,\n 314,\n 315,\n 316,\n 317,\n 318,\n 319,\n 320,\n 321,\n 322,\n 323,\n 324,\n 325,\n 326,\n 327,\n 328,\n 329,\n 330,\n 331,\n 332,\n 333,\n 334,\n 335,\n 336,\n 337,\n 338,\n 339,\n 340,\n 341,\n 342,\n 343,\n 344,\n 345,\n 346,\n 347,\n 348,\n 349,\n 350,\n 351,\n 352,\n 353,\n 354,\n 355,\n 356,\n 357,\n 358,\n 359,\n 360,\n 361,\n 362,\n 363,\n 364,\n 365,\n 366,\n 367,\n 368,\n 369,\n 370,\n 371,\n 372,\n 373,\n 374,\n 375,\n 376,\n 377,\n 378,\n 379,\n 380,\n 381,\n 382,\n 383,\n 384,\n 385,\n 386,\n 387,\n 388,\n 389,\n 390,\n 391,\n 392,\n 393,\n 394,\n 395,\n 396,\n 397,\n 398,\n 399,\n 400,\n 401,\n 402,\n 403,\n 404,\n 405,\n 406,\n 407,\n 408,\n 409,\n 410,\n 411,\n 412,\n 413,\n 414,\n 415,\n 416,\n 417,\n 418,\n 419,\n 420,\n 421,\n 422,\n 423,\n 424,\n 425,\n 426,\n 427,\n 428,\n 429,\n 430,\n 431,\n 432,\n 433,\n 434,\n 435,\n 436,\n 437,\n 438,\n 439,\n 440,\n 441,\n 442,\n 443,\n 444,\n 445,\n 446,\n 447,\n 448,\n 449,\n 450,\n 451,\n 452,\n 453,\n 454,\n 455,\n 456,\n 457,\n 458,\n 459,\n 460,\n 461,\n 462,\n 463,\n 464,\n 465,\n 466,\n 467,\n 468,\n 469,\n 470,\n 471,\n 472,\n 473,\n 474,\n 475,\n 476,\n 477,\n 478,\n 479,\n 480,\n 481,\n 482,\n 483,\n 484,\n 485,\n 486,\n 487,\n 488,\n 489,\n 490,\n 491,\n 492,\n 493,\n 494,\n 495,\n 496,\n 497,\n 498,\n 499,\n 500,\n 501,\n 502,\n 503,\n 504,\n 505,\n 506,\n 507,\n 508,\n 509,\n 510,\n 511,\n 512,\n 513,\n 514,\n 515,\n 516,\n 517,\n 518,\n 519,\n 520,\n 521,\n 522,\n 523,\n 524,\n 525,\n 526,\n 527,\n 528,\n 529,\n 530,\n 531,\n 532,\n 533,\n 534,\n 535,\n 536,\n 537,\n 538,\n 539,\n 540,\n 541,\n 542,\n 543,\n 544,\n 545,\n 546,\n 547,\n 548,\n 549,\n 550,\n 551,\n 552,\n 553,\n 554,\n 555,\n 556,\n 557,\n 558,\n 559,\n 560,\n 561,\n 562,\n 563,\n 564,\n 565,\n 566,\n 567,\n 568,\n 569,\n 570,\n 571,\n 572,\n 573,\n 574,\n 575,\n 576,\n 577,\n 578,\n 579,\n 580,\n 581,\n 582,\n 583,\n 584,\n 585,\n 586,\n 587,\n 588,\n 589,\n 590,\n 591,\n 592,\n 593,\n 594,\n 595,\n 596,\n 597,\n 598,\n 599,\n 600,\n 601,\n 602,\n 603,\n 604,\n 605,\n 606,\n 607,\n 608,\n 609,\n 610,\n 611,\n 612,\n 613,\n 614,\n 615,\n 616,\n 617,\n 618,\n 619,\n 620,\n 621,\n 622,\n 623,\n 624,\n 625,\n 626,\n 627,\n 628,\n 629,\n 630,\n 631,\n 632,\n 633,\n 634,\n 635,\n 636,\n 637,\n 638,\n 639,\n 640,\n 641,\n 642,\n 643,\n 644,\n 645,\n 646,\n 647,\n 648,\n 649,\n 650,\n 651,\n 652,\n 653,\n 654,\n 655,\n 656,\n 657,\n 658,\n 659,\n 660,\n 661,\n 662,\n 663,\n 664,\n 665,\n 666,\n 667,\n 668,\n 669,\n 670,\n 671,\n 672,\n 673,\n 674,\n 675,\n 676,\n 677,\n 678,\n 679,\n 680,\n 681,\n 682,\n 683,\n 684,\n 685,\n 686,\n 687,\n 688,\n 689,\n 690,\n 691,\n 692,\n 693,\n 694,\n 695,\n 696,\n 697,\n 698,\n 699,\n 700,\n 701,\n 702,\n 703,\n 704,\n 705,\n 706,\n 707,\n 708,\n 709,\n 710,\n 711,\n 712,\n 713,\n 714,\n 715,\n 716,\n 717,\n 718,\n 719,\n 720,\n 721,\n 722,\n 723,\n 724,\n 725,\n 726,\n 727,\n 728,\n 729,\n 730,\n 731,\n 732,\n 733,\n 734,\n 735,\n 736,\n 737,\n 738,\n 739,\n 740,\n 741,\n 742,\n 743,\n 744,\n 745,\n 746,\n 747,\n 748,\n 749,\n 750,\n 751,\n 752,\n 753,\n 754,\n 755,\n 756,\n 757,\n 758,\n 759,\n 760,\n 761,\n 762,\n 763,\n 764,\n 765,\n 766,\n 767,\n 768,\n 769,\n 770,\n 771,\n 772,\n 773,\n 774,\n 775,\n 776,\n 777,\n 778,\n 779,\n 780,\n 781,\n 782,\n 783,\n 784,\n 785,\n 786,\n 787,\n 788,\n 789,\n 790,\n 791,\n 792,\n 793,\n 794,\n 795,\n 796,\n 797,\n 798,\n 799,\n 800,\n 801,\n 802,\n 803,\n 804,\n 805,\n 806,\n 807,\n 808,\n 809,\n 810,\n 811,\n 812,\n 813,\n 814,\n 815,\n 816,\n 817,\n 818,\n 819,\n 820,\n 821,\n 822,\n 823,\n 824,\n 825,\n 826,\n 827,\n 828,\n 829,\n 830,\n 831,\n 832,\n 833,\n 834,\n 835,\n 836,\n 837,\n 838,\n 839,\n 840,\n 841,\n 842,\n 843,\n 844,\n 845,\n 846,\n 847,\n 848,\n 849,\n 850,\n 851,\n 852,\n 853,\n 854,\n 855,\n 856,\n 857,\n 858,\n 859,\n 860,\n 861,\n 862,\n 863,\n 864,\n 865,\n 866,\n 867,\n 868,\n 869,\n 870,\n 871,\n 872,\n 873,\n 874,\n 875,\n 876,\n 877,\n 878,\n 879,\n 880,\n 881,\n 882,\n 883,\n 884,\n 885,\n 886,\n 887,\n 888,\n 889,\n 890,\n 891,\n 892,\n 893,\n 894,\n 895,\n 896,\n 897,\n 898,\n 899,\n 900,\n 901,\n 902,\n 903,\n 904,\n 905,\n 906,\n 907,\n 908,\n 909,\n 910,\n 911,\n 912,\n 913,\n 914,\n 915,\n 916,\n 917,\n 918,\n 919,\n 920,\n 921,\n 922,\n 923,\n 924,\n 925,\n 926,\n 927,\n 928,\n 929,\n 930,\n 931,\n 932,\n 933,\n 934,\n 935,\n 936,\n 937,\n 938,\n 939,\n 940,\n 941,\n 942,\n 943,\n 944,\n 945,\n 946,\n 947,\n 948,\n 949,\n 950,\n 951,\n 952,\n 953,\n 954,\n 955,\n 956,\n 957,\n 958,\n 959,\n 960,\n 961,\n 962,\n 963,\n 964,\n 965,\n 966,\n 967,\n 968,\n 969,\n 970,\n 971,\n 972,\n 973,\n 974,\n 975,\n 976,\n 977,\n 978,\n 979,\n 980,\n 981,\n 982,\n 983,\n 984,\n 985,\n 986,\n 987,\n 988,\n 989,\n 990,\n 991,\n 992,\n 993,\n 994,\n 995,\n 996,\n 997,\n 998,\n 999,\n ...]"
     },
     "metadata": {},
     "execution_count": 222
    }
   ],
   "source": [
    "list(range(0,len(X8)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}